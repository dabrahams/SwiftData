{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(url: \"https://github.com/mxcl/Path.swift\", from: \"0.16.1\")\n",
      "\t\tPath\n",
      "\t.package(path: \"~/git/SwiftData\")\n",
      "\t\tBatcher\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmprjnwpybx/swift-install\n",
      "warning: /home/sgugger/swift/usr/bin/swiftc: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swiftc)\n",
      "/home/sgugger/swift/usr/bin/swift: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swift)\n",
      "warning: /home/sgugger/swift/usr/bin/swiftc: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swiftc)\n",
      "/home/sgugger/swift/usr/bin/swift: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swift)\n",
      "warning: /home/sgugger/swift/usr/bin/swiftc: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swiftc)\n",
      "/home/sgugger/swift/usr/bin/swift: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swift)\n",
      "/home/sgugger/swift/usr/bin/swiftc: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swiftc)\n",
      "/home/sgugger/swift/usr/bin/swiftc: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swiftc)[1/6] Compiling Batcher TensorPair.swift\n",
      "/home/sgugger/swift/usr/bin/swift: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swift)\n",
      "[2/6] Compiling Batcher Batcher.swift\n",
      "/home/sgugger/swift/usr/bin/swift: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swift)\n",
      "[3/6] Compiling Batcher Dataset.swift\n",
      "/home/sgugger/swift/usr/bin/swift: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swift)\n",
      "[4/6] Compiling Batcher Utils.swift\n",
      "/home/sgugger/swift/usr/bin/swift: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swift)\n",
      "[5/7] Merging module Batcher\n",
      "/home/sgugger/swift/usr/bin/swift: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swift)\n",
      "/home/sgugger/swift/usr/bin/swiftc: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swiftc)[6/8] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
      "/home/sgugger/swift/usr/bin/swift: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swift)\n",
      "[7/9] Merging module jupyterInstalledPackages\n",
      "/home/sgugger/swift/usr/bin/swift: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swift)\n",
      "/home/sgugger/swift/usr/bin/swiftc: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swiftc)\n",
      "/home/sgugger/swift/usr/bin/swift-autolink-extract: /home/sgugger/anaconda3/lib/libuuid.so.1: no version information available (required by /home/sgugger/swift/usr/bin/swift-autolink-extract)\n",
      "[8/8] Linking libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-location $cwd/swift-install\n",
    "%install '.package(url: \"https://github.com/mxcl/Path.swift\", from: \"0.16.1\")' Path\n",
    "%install '.package(path: \"~/git/SwiftData\")' Batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Path\n",
    "import TensorFlow\n",
    "import Batcher\n",
    "import Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the language model dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A language model task is to guess the next word in a stream of texts. When having a list of tokenized and numericalized texts, we usually concatenate them all together in one big stream, separate it in the desired numbers of batches (which are `batchSize` chunks of continuous texts) then read through those `sequenceLength` at a time.\n",
    "\n",
    "Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "let items = [[0,1,2,3,4],[5,6,7,8,9,10],[11,12,13,14,15,16,17,18],[19,20],[21,22]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "let dataset = LanguageModelDataset(openItem: { $0 }, batchSize: 4, sequenceLength: 3, items: items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our stream is the sequence of integers from 0 to 22. With a batchsize of 4, we split it in four chunks which are:\n",
    "```\n",
    "0,1,2,3,4\n",
    "5,6,7,8,9\n",
    "10,11,12,13,14\n",
    "15,16,17,18,19\n",
    "```\n",
    "The last three bits of the stream are thrown away because we don't have a round multiple of 4.\n",
    "\n",
    "Then if read with a sequenceLength of 3, the first batch has for input\n",
    "```\n",
    "0,1,2\n",
    "5,6,7\n",
    "10,11,12\n",
    "15,16,17\n",
    "```\n",
    "and for target the next words:\n",
    "```\n",
    "1,2,3\n",
    "6,7,8\n",
    "11,12,13\n",
    "16,17,18\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put our dataset in a batcher to check it does all of this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "let batcher = Batcher(on: dataset, batchSize: 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorPair<Int32, Int32>(input: [[ 0,  1,  2],\r\n",
      " [ 5,  6,  7],\r\n",
      " [10, 11, 12],\r\n",
      " [15, 16, 17]], target: [[ 1,  2,  3],\r\n",
      " [ 6,  7,  8],\r\n",
      " [11, 12, 13],\r\n",
      " [16, 17, 18]])\r\n",
      "TensorPair<Int32, Int32>(input: [[ 3,  4],\r\n",
      " [ 8,  9],\r\n",
      " [13, 14],\r\n",
      " [18, 19]], target: [[ 4,  5],\r\n",
      " [ 9, 10],\r\n",
      " [14, 15],\r\n",
      " [19, 20]])\r\n"
     ]
    }
   ],
   "source": [
    "for x in batcher { print(x) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first batch is as expected, and the second one has only a sequence length of 2 because our big chunks of text have a length of 5 here.\n",
    "\n",
    "Behind the scenes, `LanguageModelDataset` implements a new collection which has the proper length and subscrit, to return the pair input/target of text (and not the raw texts of varying lengths)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the shuffle enabled, the texts are shuffled before being concatenated to form the stream. We just need to use `languageModelSample` as a `sampleIndices` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "let batcher = Batcher(on: dataset, batchSize: 4, shuffle: true, sampleIndices: languageModelSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorPair<Int32, Int32>(input: [[19, 20,  0],\r\n",
      " [ 3,  4, 11],\r\n",
      " [14, 15, 16],\r\n",
      " [ 5,  6,  7]], target: [[20,  0,  1],\r\n",
      " [ 4, 11, 12],\r\n",
      " [15, 16, 17],\r\n",
      " [ 6,  7,  8]])\r\n",
      "TensorPair<Int32, Int32>(input: [[ 1,  2],\r\n",
      " [12, 13],\r\n",
      " [17, 18],\r\n",
      " [ 8,  9]], target: [[ 2,  3],\r\n",
      " [13, 14],\r\n",
      " [18,  5],\r\n",
      " [ 9, 10]])\r\n"
     ]
    }
   ],
   "source": [
    "for x in batcher { print(x) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not dealing with tokenization here so I'm using fastai v2 to tokenize the imdb dataset for me. To get this, install [fastai v2](https://github.com/fastai/fastai2) then run anywhere the following script\n",
    "```\n",
    "from fastai2.text.all import *\n",
    "path = untar_data(URLs.IMDB)\n",
    "tokenize_folder(path)\n",
    "```\n",
    "\n",
    "It will create the following folder with one file per tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "let dataPath = Path.home/\".fastai\"/\"data\"\n",
    "let path = dataPath/\"imdb_tok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "let fnames = collectFiles(under: path/\"train\", recurse: true, filtering: [\"txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each text file, the tokens are separated by texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "func readTokenizedText(_ fname: Path, separator: Character = \" \") -> [String] {\n",
    "    let text = try! String(contentsOf: URL(fileURLWithPath: fname.string), encoding: .utf8)\n",
    "    return text.split(separator: separator).map { String($0) }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a vocabulary where we will throw away the rare words, we need to determine how many each of them is present. The following functions counts that for us and also returns the lengths of each text (will save time after)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "func countTokens(_ fnames: [Path]) -> ([Int], [String:Int]) {\n",
    "    var counts: [String:Int] = [:]\n",
    "    var lengths: [Int] = []\n",
    "    for fname in fnames {\n",
    "        let tokens = readTokenizedText(fname)\n",
    "        lengths.append(tokens.count)\n",
    "        for t in tokens {\n",
    "            counts[t] = (counts[t] ?? 0) + 1\n",
    "        }\n",
    "    }\n",
    "    return (lengths,counts)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (lengths, counts) = countTokens(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the following function will create a vocabyulary containing all the most frequent words up to `maxCount`, and with a minimum frequency of `minFreq` (NB: a language model can barely learn anything about words rarely present in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "func makeVocab(_ counts: [String:Int], minFreq: Int = 2, maxCount: Int = 60000) \n",
    "-> (itos: [Int:String], stoi: [String:Int]) {\n",
    "    let withoutSpec = counts.filter { $0.0 != \"xxunk\" && $0.0 != \"xxpad\" }\n",
    "    let sorted = withoutSpec.sorted { $0.1 > $1.1 }\n",
    "    var itos: [Int:String] = [0:\"xxunk\", 1:\"xxpad\"]\n",
    "    var stoi: [String:Int] = [\"xxunk\":0, \"xxpad\":1]\n",
    "    for (i,x) in sorted.enumerated() {\n",
    "        if i+2 >= maxCount || x.1 < minFreq { break }\n",
    "        itos[i+2] = (x.0)\n",
    "        stoi[x.0] = i+2\n",
    "    }\n",
    "    return (itos: itos, stoi: stoi)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "let vocab = makeVocab(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our `vocab`, we can then numericalize each tokenized text, e.g. convert an array of strings to an array of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "func numericalize(_ tokens: [String], with stoi: [String:Int]) -> [Int] {\n",
    "    return tokens.map { stoi[$0] ?? 0 }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let tst = readTokenizedText(fnames[0])\n",
    "tst.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model batcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a `LanguageModelDataset` from all our filenames, providing a function that can read them. Since it will need all the lengths of every sample to work, we can provide the array of lengths of each text to speed up the init (if we don't, it will make a pass over the dataset to compute them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "let dataset = LanguageModelDataset(\n",
    "    openItem: { numericalize(readTokenizedText($0), with: vocab.stoi) }, \n",
    "    batchSize: 64, \n",
    "    sequenceLength: 72, \n",
    "    items: fnames, \n",
    "    lengths: lengths\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can batch our samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "let batcher = Batcher(on: dataset, batchSize: 64, numWorkers: 4, shuffle: true, sampleIndices: languageModelSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "let b = batcher.first {_ in true}!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 72] [64, 72]\r\n"
     ]
    }
   ],
   "source": [
    "print(b.input.shape,b.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, we can use the same `basicDataset` as before, the only difference is that since the texts are of different lengths, we will need to pad them to the same size before collating them, using `padSamples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "let labelToInt: [String: Int] = [\"neg\":0, \"pos\":1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "let dataset = basicDataset(\n",
    "    from: fnames, \n",
    "    toInput: { Tensor<Int32>(numericalize(readTokenizedText($0), with: vocab.stoi).map { Int32($0) }) }, \n",
    "    toTarget: { Tensor<Int32>(Int32(labelToInt[$0.parent.basename()]!)) }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "let batcher = Batcher(on: dataset, batchSize: 64, numWorkers: 4, shuffle: true, padSamples: padInputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "let b = batcher.first {_ in true}!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 1002] [64]\r\n"
     ]
    }
   ],
   "source": [
    "print(b.input.shape,b.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
